# AFHAM Voice Assistant - Feature Guide & Testing

**Version**: 1.1.0  
**Last Updated**: November 14, 2025

---

## ğŸ¤ Overview

The AFHAM Voice Assistant provides **real-time, bilingual voice interaction** with your personal document library. Speak naturally in Arabic or English and receive instant answers enriched with citations from uploaded documents.

### Key Features

âœ… **Real-Time Streaming** - Instant speech recognition and response  
âœ… **Document-Grounded Answers** - Automatically pulls context from your uploads  
âœ… **Bilingual Support** - Seamless Arabic/English conversation  
âœ… **Citation-Rich Responses** - References specific documents and pages  
âœ… **Auto-Sync** - All Documents and Workspace uploads are instantly available  

---

## ğŸ—ï¸ Architecture

### Data Flow

```
User Speech (AR/EN)
    â†“
VoiceAssistantManager (Speech-to-Text)
    â†“
VoiceAssistantView.processVoiceInput()
    â†“
GeminiFileSearchManager.queryDocuments(question, language)
    â†“
Gemini API + Document Index
    â†“
Response with Citations
    â†“
VoiceAssistantManager.speak() (Text-to-Speech)
    â†“
Audio Output (AR/EN)
```

### Key Components

#### 1. **VoiceAssistantView** (`AFHAM/Features/Chat/afham_chat.swift:368`)
- Main UI for voice interaction
- Handles speech visualization
- Processes recognized text
- Displays responses with citations

#### 2. **VoiceAssistantManager** (`AFHAM/Core/afham_main.swift:418`)
- Manages Speech Recognition (SFSpeechRecognizer)
- Handles Text-to-Speech (AVSpeechSynthesizer)
- Real-time audio processing
- Language-aware voice selection

#### 3. **GeminiFileSearchManager** (`AFHAM/Core/afham_main.swift`)
- Document indexing and retrieval
- Query processing with context
- Citation extraction
- Bilingual response generation

### Document Integration

```swift
// From VoiceAssistantView.processVoiceInput()
let (answer, citations) = try await geminiManager.queryDocuments(
    question: voiceManager.recognizedText,
    language: isArabic ? "ar" : "en"
)

// Auto-speak response in user's language
voiceManager.speak(
    text: answer,
    language: isArabic ? "ar-SA" : "en-US"
)
```

**Key Point**: Any document uploaded via:
- ğŸ“„ Documents tab
- ğŸ¨ Modular Workspace
- ğŸ“¸ Intelligent Capture

...is immediately available for voice queries. **No extra setup required.**

---

## ğŸ§ª Testing Guide

### Prerequisites

1. **Device/Simulator**:
   - Physical device recommended (simulator has microphone limitations)
   - **Minimum**: iPhone 12 or newer
   - **Recommended**: iPhone 14 Pro or newer for optimal performance
   - **iOS Version**: 17.0 or later
2. **Permissions**: Microphone and Speech Recognition enabled
3. **Network**: Wi-Fi or cellular for Gemini API calls
4. **Language**: System language set (Settings â†’ General â†’ Language & Region)

### Test Scenario 1: Basic Voice Query

**Steps:**
1. Launch AFHAM
2. Navigate to **Voice** tab (ğŸ¤)
3. Tap the microphone button
4. Speak: "What is AFHAM?" (English) or "Ù…Ø§ Ù‡Ùˆ Ø£ÙÙ‡Ù…ØŸ" (Arabic)
5. Wait for response

**Expected Result:**
- âœ… Speech recognized and displayed
- âœ… Response appears in text
- âœ… Response auto-plays in spoken language
- âœ… Clear and trash buttons functional

### Test Scenario 2: Document-Grounded Query

**Steps:**
1. Go to **Documents** tab
2. Upload a medical document (PDF/image)
3. Wait for processing to complete
4. Switch to **Voice** tab
5. Ask: "Summarize my uploaded document" or "Ø§Ø®ØªØµØ± Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø°ÙŠ Ø±ÙØ¹ØªÙ‡"

**Expected Result:**
- âœ… Response references uploaded document
- âœ… Includes specific citations (page numbers, excerpts)
- âœ… Accurate content extraction
- âœ… Response in query language

### Test Scenario 3: Bilingual Switching

**Steps:**
1. With English locale:
   - Ask: "What documents do I have?"
   - Verify English response

2. Change app language to Arabic:
   - Settings â†’ Language â†’ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
   - Return to Voice tab
   - Ask: "Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©ØŸ"
   - Verify Arabic response

3. Try mixed query:
   - "Show me documents about diabetes"
   - Verify language detection and appropriate response

**Expected Result:**
- âœ… Correct language detection
- âœ… Response matches query language
- âœ… TTS voice changes with language
- âœ… Same documents accessible in both languages

### Test Scenario 4: Multi-Document Context

**Steps:**
1. Upload 3 different documents (e.g., prescription, lab report, patient history)
2. Ask contextual question: "What medications am I taking?" or "Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø£Ø¯ÙˆÙŠØ© Ø§Ù„ØªÙŠ Ø£ØªÙ†Ø§ÙˆÙ„Ù‡Ø§ØŸ"
3. Verify response synthesizes information from multiple documents

**Expected Result:**
- âœ… Combines data from all relevant documents
- âœ… Lists citations from each source
- âœ… Coherent multi-source answer

### Test Scenario 5: Workspace Integration

**Steps:**
1. Go to **Workspace** tab
2. Create a presentation from a document
3. Switch to **Voice** tab
4. Ask: "What presentations have I created?"

**Expected Result:**
- âœ… Lists workspace outputs
- âœ… References transformation pipelines
- âœ… Provides metadata about created assets

### Test Scenario 6: Error Handling

**Steps:**
1. **No Documents**: Ask document question with empty library
   - Expected: Helpful message explaining no documents available

2. **No Network**: Disable Wi-Fi/cellular, ask question
   - Expected: Error message in appropriate language

3. **Unclear Speech**: Mumble or speak unclearly
   - Expected: "I didn't catch that" message

4. **Interrupted Speech**: Start speaking, then stop abruptly
   - Expected: Graceful timeout and ready for new input

### Test Scenario 7: Performance

**Steps:**
1. Upload large document (50+ pages)
2. Ask complex question requiring full document analysis
3. Measure response time

**Expected Result:**
- âœ… Response within 3-5 seconds for indexed documents
- âœ… Streaming response if available
- âœ… Progress indicator during processing
- âœ… No UI freezing

---

## ğŸ”§ Build Validation

### Local Build Check

```bash
# Clean build
xcodebuild clean build \
  -scheme AFHAM \
  -project AFHAM.xcodeproj \
  -sdk iphonesimulator \
  -destination 'platform=iOS Simulator,name=iPhone 15 Pro' \
  CODE_SIGN_IDENTITY="" \
  CODE_SIGNING_REQUIRED=NO

# Expected Output:
# ** BUILD SUCCEEDED **
```

### Run on Device

```bash
# For physical device testing (requires developer account)
xcodebuild build \
  -scheme AFHAM \
  -project AFHAM.xcodeproj \
  -sdk iphoneos \
  -configuration Debug \
  -destination 'platform=iOS,name=Your iPhone' \
  CODE_SIGN_STYLE=Automatic \
  DEVELOPMENT_TEAM=YOUR_TEAM_ID
```

### Verify Voice Components

```bash
# Check for voice-related symbols in build
nm -g DerivedData/Build/Products/Debug-iphonesimulator/AFHAM.app/AFHAM | \
  grep -i "voice\|speech\|audio"

# Should include:
# VoiceAssistantManager
# VoiceAssistantView
# processVoiceInput
# SFSpeechRecognizer
# AVSpeechSynthesizer
```

---

## ğŸ“Š Testing Checklist

### Functional Tests
- [ ] Speech recognition activates on button press
- [ ] Real-time transcription displays
- [ ] Query sent to GeminiFileSearchManager
- [ ] Response retrieved with citations
- [ ] Text-to-Speech plays automatically
- [ ] Clear button resets state
- [ ] Trash button clears history

### Language Tests
- [ ] English voice input â†’ English response
- [ ] Arabic voice input â†’ Arabic response
- [ ] Mixed language queries handled gracefully
- [ ] TTS voice quality appropriate for language
- [ ] RTL text displayed correctly for Arabic

### Integration Tests
- [ ] Documents tab uploads reflected in voice queries
- [ ] Workspace outputs queryable via voice
- [ ] Intelligent Capture scans accessible
- [ ] Multi-document context synthesis works
- [ ] Citations link to correct source documents

### Performance Tests
- [ ] Response time < 5 seconds for simple queries
- [ ] No memory leaks after extended use
- [ ] Background/foreground transitions stable
- [ ] Concurrent uploads don't block voice
- [ ] Large document libraries perform well

### Edge Cases
- [ ] No documents uploaded â†’ helpful message
- [ ] Network offline â†’ error handling
- [ ] Microphone permission denied â†’ prompt
- [ ] Speech recognition unavailable â†’ fallback
- [ ] Empty/unclear speech â†’ retry prompt

---

## ğŸ› Known Issues & Workarounds

### Issue 1: Delayed First Response
**Symptom**: First voice query takes 10+ seconds  
**Cause**: Cold start of speech recognizer  
**Workaround**: Pre-warm by tapping mic button on app launch

### Issue 2: Background Audio Conflicts
**Symptom**: TTS doesn't play if music is playing  
**Cause**: Audio session configuration  
**Workaround**: Pause other audio before using voice

### Issue 3: Arabic Accent Variation
**Symptom**: Recognition accuracy varies by dialect  
**Cause**: SFSpeechRecognizer training data  
**Workaround**: Speak Modern Standard Arabic for best results

---

## ğŸ“± User-Facing How-To Card

### In-App Guide Content

**Title**: ğŸ¤ Voice Assistant Quick Start

**Body**:
```
Talk to Your Documents

1. Upload documents via Documents, Capture, or Workspace tabs
   â†’ Automatically indexed for voice queries

2. Tap the Voice tab (ğŸ¤)
   â†’ Press microphone to start speaking

3. Ask questions in Arabic or English
   â†’ "Summarize my lab results"
   â†’ "Ø§Ø®ØªØµØ± Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ÙØ­ÙˆØµØ§Øª"

4. Get instant answers with citations
   â†’ References your personal document library
   â†’ Speaks response in your language

Tips:
â€¢ All uploads auto-sync to voice assistant
â€¢ No extra setup required
â€¢ Switch languages anytime
â€¢ Ask follow-up questions naturally
```

**Visual**: Screenshot showing Voice tab with sample query/response

---

## ğŸ¬ Demo Recording Guide

### Bilingual Demo Script

#### Scene 1: English Workflow (30 seconds)
```
[Screen: Documents tab]
"I'm uploading my prescription..."

[Screen: Voice tab, tap mic]
"What medications am I taking?"

[Show: Response with citations]
"AFHAM found my prescription and listed all medications with dosages."
```

#### Scene 2: Arabic Workflow (30 seconds)
```
[Screen: Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø®ØªØ¨Ø± tab]
"Ø£Ù†Ø§ Ø£Ø±ÙØ¹ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ÙØ­ÙˆØµØ§Øª..."

[Screen: Voice tab, Ø§Ø¶ØºØ· Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†]
"Ù…Ø§ Ù‡ÙŠ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ÙØ­ÙˆØµØ§ØªØŸ"

[Show: Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù…Ø¹ Ø§Ù„Ù…ØµØ§Ø¯Ø±]
"Ø£ÙÙ‡Ù… ÙˆØ¬Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙˆØ£Ø¹Ø·Ø§Ù†ÙŠ Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„ÙƒØ§Ù…Ù„."
```

#### Scene 3: Context Switching (20 seconds)
```
[Show: Multiple documents in library]
"I have prescriptions, lab results, and medical history uploaded."

[Voice query]
"Give me a health summary based on all my documents."

[Show: Comprehensive response synthesizing all sources]
```

### Recording Specs
- **Resolution**: 1080p minimum
- **Format**: MP4 (H.264)
- **Duration**: 60-90 seconds total
- **Captions**: Arabic and English subtitles
- **Audio**: Clear voice, ambient noise < -40dB

---

## ğŸ”— Related Documentation

- [AFHAM User Guide](QUICK_START.md)
- [Document Upload Guide](BUILD_GUIDE.md)
- [Modular Workspace Guide](AFHAM/Features/DocsWorkspace/ModularCanvas/README.md)
- [Intelligent Capture Guide](AFHAM_INTELLIGENT_CAPTURE_README.md)

---

## ğŸš€ Next Steps

### For Developers
1. Run build validation on local machine
2. Test on physical iOS device
3. Record demo clips for marketing
4. Add how-to card to Voice tab UI

### For Testers
1. Complete testing checklist above
2. Report issues to GitHub
3. Provide feedback on UX
4. Test with real medical documents (anonymized)

### For Product Team
1. Review demo clips
2. Create onboarding video
3. Update App Store screenshots
4. Prepare release announcement

---

**Questions?** Contact: support@brainsait.com

